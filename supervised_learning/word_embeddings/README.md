# Word Embeddings Project

## Overview

This project explores various word embedding techniques used in Natural Language Processing (NLP). It includes implementations of popular models such as Word2Vec, GloVe, FastText, and ELMo.

## Features

- **Word2Vec**: Implementations of CBOW and Skip-gram models.
- **GloVe**: Global Vectors for Word Representation.
- **FastText**: Character n-grams for better handling of out-of-vocabulary words.
- **ELMo**: Contextualized word embeddings.

## Requirements

- Python 3.x
- Required libraries: `gensim`, `numpy`, `scikit-learn`, etc.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/word-embeddings.git
   cd word-embeddings
2. Create a virtual environment:
    ```bash
    python -m venv env
source env/Scripts/activate  # On Windows use: .\env\Scripts\activate
3. Install the required packages:
    ```bash
    pip install -r requirements.txt
